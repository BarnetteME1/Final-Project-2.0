{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I need to do is import the libraries I will use. The ones I use most are pandas and XGBoost. Pandas reads in the files and converts them into a dataframe, while XGBoost is what I use to write my predictions program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('SF_crime/test.csv', index_col='Id')\n",
    "test = test.rename(columns={'X': 'Longitude', \"Y\": \"Latitude\"})\n",
    "test.Dates = pd.to_datetime(test.Dates)\n",
    "test['month'] = test.Dates.dt.month\n",
    "test['time'] = test.Dates.dt.time\n",
    "test['day'] = test.Dates.dt.day\n",
    "test = test.drop(['Dates'], axis=1)\n",
    "test_keep = test\n",
    "crime_in_sf = pd.read_csv('SF_crime/train.csv')\n",
    "crime_in_sf.Dates = pd.to_datetime(crime_in_sf.Dates)\n",
    "crime_in_sf = crime_in_sf.rename(columns={'X': 'Longitude', \"Y\": \"Latitude\",})\n",
    "crime_in_sf = crime_in_sf.drop(['Resolution', 'Descript'], axis=1)\n",
    "crime_in_sf['month'] = crime_in_sf.Dates.dt.month\n",
    "crime_in_sf['time'] = crime_in_sf.Dates.dt.time\n",
    "crime_in_sf['day'] = crime_in_sf.Dates.dt.day\n",
    "crime_in_sf = crime_in_sf.drop(['Dates'], axis=1)\n",
    "crime_train, crime_test = train_test_split(crime_in_sf, test_size=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing I have to do is read in all the files and make any corrections to them so I can make them more readable. I change some column names (X and Y) so that they are easier to read and convert the Dates column to a datetime format so I can pull out individual years or days if I need too. I also drop two columns off of the training data as they don't influence my predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in test.columns.values:\n",
    "    if column != 'Longitude' and column != 'Latitude' and column != 'month' and column != 'day':\n",
    "        le.fit(test[column])\n",
    "        test[column] = le.transform(test[column])\n",
    "\n",
    "for column in crime_in_sf.columns.values:\n",
    "    if column != 'Longitude' and column != 'Latitude' and column != 'month' and column != 'day':\n",
    "        le.fit(crime_in_sf[column])\n",
    "        crime_train[column] = le.transform(crime_train[column])\n",
    "\n",
    "for column in crime_in_sf.columns.values:\n",
    "    if column != 'Longitude' and column != 'Latitude' and column != 'month' and column != 'day':\n",
    "        le.fit(crime_in_sf[column])        \n",
    "        crime_test[column] = le.transform(crime_test[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I take the information, except for the latitude and longitude, and convert it from strings into integers. Each one is in a dictionary and stored so that they can be converted back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = crime_train.Category\n",
    "crime_train = crime_train.drop('Category', axis=1)\n",
    "\n",
    "categories2 = crime_test.Category\n",
    "crime_test = crime_test.drop('Category', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly train my data I needed to make the categories of crimes seperate from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(crime_train.as_matrix(),\n",
    "                     label=categories)\n",
    "dtest = xgb.DMatrix(crime_test.as_matrix(),\n",
    "                    label=categories2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is separated it needs to be prepared to be put into the decision tree. The first thing that needs to happen is that the information is converted from a pandas table into a matrix, and the categories need to be added in their own identifier so that the program knows what it's predicting on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = {'bst:max_depth':8, 'objective':'multi:softprob', 'num_class':39}\n",
    "param['nthread'] = 6\n",
    "param['eval_metric'] = ['mlogloss']\n",
    "evallist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "num_round = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly I need to tell the program how it's suppose to wrong and what it should use to evaluate the information. I set how large of a tree I want (the max_depth), what I want it to return (softprob), how many categories it should be in. \n",
    "I also set up the evaluation metrics that it would run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is where the program trains. As you can see the numbers are getting smaller as they go along, showing that it is getting more accurate. This will hopefully give me a better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = bst.predict(xgb.DMatrix(test.as_matrix()), output_margin=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained I convert the data I will actually predict upon into a matrix and run it through the model I just created and it returns it's predictions based off of all the descisions it had to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then put those predictions back into a DataFrame. I can easily use that to look over my data and see what it looks like. This is a good time to see if there are any trends or problems that may arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le.fit(crime_in_sf.Category)\n",
    "predictions.columns = le.inverse_transform(predictions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also relabel the information so that it has what the crimes are as opposed to simply numbers from 0-38 so that I know what the crimes that it is predicting on are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime_in_sf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what my data looked like when I fed it into my program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what my predictions look like. Several categories which numbers to identify them and a probabilities of their likelyhood for each type of crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions['Id'] = predictions.index\n",
    "\n",
    "def order(frame,var):\n",
    "    varlist =[w for w in frame.columns if w not in var]\n",
    "    frame = frame[var+varlist]\n",
    "    return frame\n",
    "\n",
    "predictions = order(predictions,['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a small definition here to add an ID column and put it on the front of my data so that it could be easily identified for the competition, and then I simply run the panel below and create a file which I can submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.to_csv('/Users/MatthewBarnette/final_project_predictions//predictions_XGB_280.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
